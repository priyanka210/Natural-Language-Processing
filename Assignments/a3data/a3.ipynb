{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304e37a2",
   "metadata": {},
   "source": [
    "# CS4765/6765 NLP Assignment 3: Word vectors\n",
    "\n",
    "In this two part assignment you will first examine and interact with word vectors. (This part of the assignment is adapted from a recent CS224N assignment at Stanford.) You will then implement a new approach to sentiment analysis.\n",
    "\n",
    "In this assignment we will use [gensim](https://radimrehurek.com/gensim/) to access and interact with word embeddings. In gensim we’ll be working with a KeyedVectors object which represents word embeddings. [Documentation for KeyedVectors is available.](https://radimrehurek.com/gensim/models/keyedvectors.html) However, this assignment description and the sample code in it might be sufficient to show you how to use a KeyedVectors object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ffb28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "model = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b65d9",
   "metadata": {},
   "source": [
    "# Part 1: Examining word vectors\n",
    "\n",
    "## Polysemy and homonymy\n",
    "\n",
    "Polysemy and homonymy are the phenomena of words having multiple meanings/senses. The nearest neighbours (under cosine similarity) for a given word can indicate whether it has multiple senses.\n",
    "\n",
    "Consider the following example which shows the top-10 most similar words for *mouse*. The \"input device\" and \"animal\" senses of *mouse* are clearly visible from the top-10 most similar words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32120e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mice', 0.7038448452949524),\n",
       " ('rat', 0.6446240544319153),\n",
       " ('rodent', 0.6280483603477478),\n",
       " ('Mouse', 0.6180493831634521),\n",
       " ('cursor', 0.6154769062995911),\n",
       " ('keyboard', 0.6149151921272278),\n",
       " ('rabbit', 0.607288658618927),\n",
       " ('cat', 0.6070616245269775),\n",
       " ('joystick', 0.5888146162033081),\n",
       " ('touchpad', 0.5878496766090393)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find words most similar using cosine similarity to \"mouse\". \n",
    "# restrict_vocab=100000 limits the results to most frequent\n",
    "# 100000 words. This avoids rare words in the output. For this\n",
    "# assignment, whenever you call most_simlilar, also pass\n",
    "# restrict_vocab=100000.\n",
    "model.most_similar('mouse', restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34e2ac",
   "metadata": {},
   "source": [
    "*Cursor*, *keyboard*, *joystick*, *touchpad* correspond to the input device sense. *Rat*, *rodent*, *rabbit*, *cat* correspond to the animal sense.\n",
    "\n",
    "\n",
    "You can observe something similar for the different senses of the word *leaves*. Find a new example that exhibits polysemy/homonymy, show its top-10 most similar words, and explain why they show that this word has multiple senses. Write your answer in the code and text boxes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0194298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lies', 0.83467036485672),\n",
       " ('lying', 0.7737990021705627),\n",
       " ('lied', 0.7100739479064941),\n",
       " ('falsehood', 0.6283779740333557),\n",
       " ('lay', 0.6210237741470337),\n",
       " ('truth', 0.6181300282478333),\n",
       " ('pretend', 0.614453911781311),\n",
       " ('untruth', 0.6050377488136292),\n",
       " ('deceit', 0.6038239598274231),\n",
       " ('deception', 0.595779538154602)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "model.most_similar('lie',restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86905a9c",
   "metadata": {},
   "source": [
    "The word lie has multiple senses with similarities to words like \"falsehood\", \"deceit\", and \"lay\". For instance, falsehood and deceit describe the act of being untruthful, while lay refers to the act of resting in a horizontal position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90ead9",
   "metadata": {},
   "source": [
    "## Synonyms and antonyms\n",
    "\n",
    "Find three words (w1 , w2 , w3) such that w1 and w2 are synonyms (i.e., have roughly the same meaning), and w1 and w3 are antonyms (i.e., have opposite meanings), but the similarity between w1 and w3 > the similarity between w1 and w2. Note that this should be counter to your expectations, because synonyms (which mean roughly the same thing) would be expected to be more similar than antonyms (which have opposite meanings). Explain why you think this unexpected situation might have occurred.\n",
    "\n",
    "Here is an example. w1 = *happy*, w2 = *cheerful*, and w3 = *sad*. (You will need to find a different example for your report.) Notice that the antonyms *happy* and *sad* are (slightly) more similar than the (near) synonyms *happy* and *cheerful*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45f158c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68476284"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the cosine similarity between \"happy\" and \"cheerful\"\n",
    "model.similarity('happy', 'cheerful')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fd9873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69010293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and between \"happy\" and \"sad\".\n",
    "model.similarity('happy', 'sad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a061ca65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51089025"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "model.similarity('like','adore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9dd9669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5742829"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('like','hate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c15629",
   "metadata": {},
   "source": [
    "\"like\" and \"adore\" show positive emotions. \"like\" and \"hate\" are antonyms. Here, the similarity between \"like\" and \"adore\" is lower than the similarity between \"like\" and \"hate\". In Word2Vec model, if two words often appear together, their vector similarity increases. The words \"like\" and \"adore\" may not be appearing as frequently and together as the words \"like\" and \"hate\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72540c",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "Analogies such as man is to king as woman is to X can be solved using word embeddings. This analogy can be expressed as X = woman + king − man. The following code snippet shows how to solve this analogy with gensim. Notice that the model gets it correct! I.e., *queen* is the most similar word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24c757c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7786749005317688),\n",
       " ('monarch', 0.6666999459266663),\n",
       " ('princess', 0.653827428817749),\n",
       " ('kings', 0.6497675180435181),\n",
       " ('queens', 0.6284460425376892),\n",
       " ('prince', 0.6235989928245544),\n",
       " ('ruler', 0.5971586108207703),\n",
       " ('kingship', 0.5883600115776062),\n",
       " ('lady', 0.5851913094520569),\n",
       " ('royal', 0.5821066498756409)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the model's predictions for the solution to the analogy\n",
    "# \"man\" is to \"king\" as \"woman\" is to X\n",
    "model.most_similar(positive=['woman', 'king'],\n",
    "                   negative=['man'],\n",
    "                   restrict_vocab=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416e879",
   "metadata": {},
   "source": [
    "### Correct analogy\n",
    "\n",
    "Find a new analogy that the model is able to answer correctly (i.e., the most-similar word is the solution to the analogy). Explain briefly why the analogy holds. For the above example, this explanation would be something along the lines of a king is a ruler who is a man and a queen is a ruler who is a woman.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f75da5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cooking', 0.6424155235290527),\n",
       " ('restaurant', 0.5705956220626831),\n",
       " ('kitchen', 0.5680593252182007),\n",
       " ('chefs', 0.558533251285553),\n",
       " ('Chef', 0.5407431721687317),\n",
       " ('cook', 0.5393264889717102),\n",
       " ('cookery', 0.5337901711463928),\n",
       " ('repainting', 0.5258774757385254),\n",
       " ('cooks', 0.5199151039123535),\n",
       " ('decorating', 0.5195807218551636)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "model.most_similar(positive=['chef', 'painting'],\n",
    "                   negative=['artist'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdf5fa",
   "metadata": {},
   "source": [
    "Artist is someone who paints and paintings are their creative innovation where they show their talent. The analogy of \"artist\" to \"painting\" relates to the anlogy of \"chef\" to \"cooking\" as it represents a form of art and creativity for a chef."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137368b",
   "metadata": {},
   "source": [
    "### Incorrect analogy\n",
    "\n",
    "Find a new analogy that the model is not able to answer correctly. Again explain briefly why the analogy holds. For example, here is an analogy that the model does not answer correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d4e4bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cups', 0.5481787919998169),\n",
       " ('coffee', 0.5461026430130005),\n",
       " ('beverage', 0.5460603833198547),\n",
       " ('drink', 0.5451807975769043),\n",
       " ('tea', 0.53434818983078),\n",
       " ('foods', 0.5310320854187012),\n",
       " ('drinks', 0.516447901725769),\n",
       " ('beverages', 0.5022991299629211),\n",
       " ('milk', 0.4976045787334442),\n",
       " ('non-food', 0.4929129481315613)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the model's predictions for the solution to the analogy\n",
    "# \"plate\" is to \"food\" as \"cup\" is to X\n",
    "model.most_similar(positive=['cup', 'food'],\n",
    "                   negative=['plate'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70dad5",
   "metadata": {},
   "source": [
    "A plate is used to serve food as a cup is used to serve a drink, but the model does not predict *drink*, or a similar term, as the most similar word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30a21892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flatbread', 0.5114241242408752),\n",
       " ('dough', 0.49446845054626465),\n",
       " ('mixture', 0.4813441038131714),\n",
       " ('molten', 0.48022177815437317),\n",
       " ('buttered', 0.47864025831222534),\n",
       " ('breads', 0.4758601486682892),\n",
       " ('illiquid', 0.4682086110115051),\n",
       " ('porous', 0.4669260084629059),\n",
       " ('liquids', 0.465412437915802),\n",
       " ('sourdough', 0.46143409609794617)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "model.most_similar(positive=['bread', 'liquid'],\n",
    "                   negative=['milk'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c636b77",
   "metadata": {},
   "source": [
    "\"milk\" is in a \"liquid\" state. The ideal output of this code is \"porous\" for the word \"bread\", but the model predicts flatbread as the most similar word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dae136",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Consider the examples below. The first shows the words that are most similar to *man* and *worker* and least similar to *woman*. The second shows the words that are most similar to *woman* and *worker* and least similar to *man*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79b1ccfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('workman', 0.7217649817466736),\n",
       " ('laborer', 0.6744564175605774),\n",
       " ('labourer', 0.6498093605041504),\n",
       " ('workers', 0.6487939357757568),\n",
       " ('foreman', 0.6226886510848999),\n",
       " ('machinist', 0.6098095178604126),\n",
       " ('employee', 0.6091086864471436),\n",
       " ('technician', 0.6029269099235535),\n",
       " ('helper', 0.5994961261749268),\n",
       " ('manager', 0.5832769274711609)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the words that are most similar to \"man\" and \"worker\" and\n",
    "# least similar to \"woman\".\n",
    "model.most_similar(positive=['man', 'worker'],\n",
    "                   negative=['woman'],\n",
    "                   restrict_vocab=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd781f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheerleaders', 0.7048168778419495),\n",
       " ('cheerleading', 0.6419737339019775),\n",
       " ('Cheerleader', 0.6108335256576538),\n",
       " ('girl', 0.5797179341316223),\n",
       " ('schoolgirl', 0.5546731948852539),\n",
       " ('Cheerleaders', 0.547419011592865),\n",
       " ('businesswoman', 0.5433568954467773),\n",
       " ('tomboy', 0.5339425802230835),\n",
       " ('mom', 0.5313844084739685),\n",
       " ('actress', 0.5302129983901978)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the words that are most similar to \"woman\" and \"worker\" and\n",
    "# least similar to \"man\".\n",
    "model.most_similar(positive=['woman', 'worker'],\n",
    "                   negative=['man'],\n",
    "                   restrict_vocab=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4f4e6",
   "metadata": {},
   "source": [
    "The output shows that *man* is associated with some stereotypically male jobs (e.g., foreman, machinist) while *woman* is associated with some stereotypically female jobs (e.g., housewife, nurse, seamstress). This indicates that there is gender bias in the word embeddings.\n",
    "\n",
    "Find a new example, using the same approach as above, that indicates that there is bias in the word embeddings. Briefly explain how the model output indicates that there is bias in the word embeddings. (You are by no means restricted to considering gender bias here. You are encouraged to explore other ways that embeddings might indicate bias.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8765625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('astute', 0.5551915764808655),\n",
       " ('smart', 0.5531858801841736),\n",
       " ('perceptive', 0.5490319728851318),\n",
       " ('unintelligent', 0.5447569489479065),\n",
       " ('brilliant', 0.5297819375991821),\n",
       " ('doctors', 0.5252407789230347),\n",
       " ('erudite', 0.5232177376747131),\n",
       " ('sane', 0.5215501189231873),\n",
       " ('clever', 0.5201746225357056),\n",
       " ('well-informed', 0.5177281498908997)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "model.most_similar(positive=['doctor', 'intelligent'],\n",
    "                   negative=['worker'],\n",
    "                   restrict_vocab=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ad1a61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unintelligent', 0.5780892372131348),\n",
       " ('smart', 0.5646598935127258),\n",
       " ('skilled', 0.5364747047424316),\n",
       " ('hard-working', 0.5275830626487732),\n",
       " ('efficient', 0.5258950591087341),\n",
       " ('intelligence', 0.523655354976654),\n",
       " ('hardworking', 0.5231699347496033),\n",
       " ('perceptive', 0.5225290656089783),\n",
       " ('adaptable', 0.5180816054344177),\n",
       " ('industrious', 0.5173808336257935)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['worker', 'intelligent'],\n",
    "                   negative=['doctor'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec7446",
   "metadata": {},
   "source": [
    "There is an occupational bias in word embeddings. If a worker is said to be intelligent, the model predicts that a doctor is asture or smart, which is similar to being intelligent. Whereas, the model predicts that a worker is unintelligent when a doctor is associated with the word intelligent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34212ddf-1d63-486c-a9e2-8c86ce814272",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis\n",
    "\n",
    "## Background and data\n",
    "\n",
    "In this part you will consider sentiment analysis of tweets. You will need the data for this assignmnet from D2L: train.docs.txt. train.classes.txt, test.docs.txt, test.classes.txt. Put those files in the same directory that you run this notebook from.\n",
    "\n",
    "train.docs.txt and test.docs.txt are training and testing tweets, respectively, in one-tweet-per-line format. These are tweets related to health care reform in the United States from early 2010. All tweets contain the hashtag #hcr. These tweets have been manually labeled as “positive”, “negative”, or “neutral”.\n",
    "\n",
    "These are real tweets. Some of the tweets contain content that you might find offensive (e.g., expletives, racist and homophobic remarks). Despite this offensive content, these tweets are still very valuable data, and building NLP systems that can operate over them is important. That is why we are working with this potentially-offensive data in this assignment.\n",
    "\n",
    "This dataset is further described in the following paper: Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Jason Baldridge. 2011. [Twitter Polarity Classification with Label Propagation over Lexical Links and the Follower Graph](https://aclanthology.org/W11-2207/). In Proceedings of the First Workshop on Unsupervised Methods in NLP. Edinburgh, Scotland.\n",
    "\n",
    "train.classes.txt and test.classes.txt contain class labels for the training and test data, 1 label per line. The labels are “positive”, “neutral”, and “negative”.\n",
    "\n",
    "## Approach\n",
    "\n",
    "We will consider sentiment analysis using an average of word embeddings document representation and a multinomial logistic regression classifier. We will compare this approach to a most-frequent class baseline.\n",
    "\n",
    "Complete the function `vec_for_doc` below. (You should not modify other parts of the\n",
    "code.) This function takes a list consisting of the tokens in a document $d$. It then returns a vector $\\vec{v}$ representing the document as the average of the embeddings for the words in the document as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "d = w_1, w_2, ... w_n\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\vec{v} = \\dfrac{\\vec{w_1} + \\vec{w_2} + ... + \\vec{w_n}}{n}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "You can then run the code to compare logistic regression using an average of word embeddings to a most-frequent class baseline. (If your implementation of `vec_for_doc` is correct, logistic regression should be the baseline in terms of accuracy (by a little bit) and in terms of F1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "855e4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this function. tokenized_doc is a list of tokens in\n",
    "# a document. Return a vector representation of the document as\n",
    "# described above.\n",
    "# Hints: \n",
    "# -You can get the vector for a word w using model[w] or\n",
    "#  model.get_vector(w)\n",
    "# -You can add vectors using + and sum, e.g.,\n",
    "#  model['cat'] + model['dog']\n",
    "#  sum([model['cat'], model['dog']])\n",
    "# -You can see the shape of a vector using model['cat'].shape\n",
    "# -The vector you return should have the same shape as a word vector \n",
    "def vec_for_doc(tokenized_doc):\n",
    "    # TODO: Add your code here\n",
    "    \n",
    "    word_shape = model['farmer'].shape\n",
    "    word_v = np.zeros(word_shape)\n",
    "    \n",
    "    for token in tokenized_doc:\n",
    "        if token in model:\n",
    "            word_v += model[token]\n",
    "        \n",
    "    return(word_v/len(tokenized_doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b69d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get the train and test documents and classes. File formats\n",
    "# are similar to assignment 2.\n",
    "train_texts_fname = 'train.docs.txt'\n",
    "train_klasses_fname = 'train.classes.txt'\n",
    "test_texts_fname = 'test.docs.txt'\n",
    "test_klasses_fname = 'test.classes.txt'\n",
    "\n",
    "train_texts = [x.strip() for x in open(train_texts_fname,\n",
    "                                       encoding='utf8')]\n",
    "train_klasses = [x.strip() for x in open(train_klasses_fname,\n",
    "                                         encoding='utf8')]\n",
    "test_texts = [x.strip() for x in open(test_texts_fname,\n",
    "                                      encoding='utf8')]\n",
    "test_klasses = [x.strip() for x in open(test_klasses_fname,\n",
    "                                        encoding='utf8')]\n",
    "\n",
    "# A simple tokenizer. Applies case folding\n",
    "def tokenize(s):\n",
    "    tokens = s.lower().split()\n",
    "    trimmed_tokens = []\n",
    "    for t in tokens:\n",
    "        if re.search('\\w', t):\n",
    "            # t contains at least 1 alphanumeric character\n",
    "            t = re.sub('^\\W*', '', t) # trim leading non-alphanumeric chars\n",
    "            t = re.sub('\\W*$', '', t) # trim trailing non-alphanumeric chars\n",
    "        trimmed_tokens.append(t)\n",
    "    return trimmed_tokens\n",
    "\n",
    "# train_vecs and test_vecs are lists; each element is a vector\n",
    "# representing a (train or test) document\n",
    "train_vecs = [vec_for_doc(tokenize(x)) for x in train_texts]\n",
    "test_vecs = [vec_for_doc(tokenize(x)) for x in test_texts]\n",
    "\n",
    "# Train logistic regression, similarly to assignment 2\n",
    "lr = LogisticRegression(multi_class='multinomial',\n",
    "                        solver='sag',\n",
    "                        penalty='l2',\n",
    "                        max_iter=1000000,\n",
    "                        random_state=0)\n",
    "lr = LogisticRegression()\n",
    "clf = lr.fit(train_vecs, train_klasses)\n",
    "results = clf.predict(test_vecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb5da740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6975\n",
      "Macro F1:  0.3875844421020532\n"
     ]
    }
   ],
   "source": [
    "# Determine accuracy and macro F1 using sklearn evaluation metrics\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "acc = sklearn.metrics.accuracy_score(test_klasses, results)\n",
    "f1 = sklearn.metrics.f1_score(test_klasses, results, average='macro')\n",
    "\n",
    "print(\"Accuracy: \", acc) \n",
    "print(\"Macro F1: \", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "076ffb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy:  0.67\n",
      "Baseline macro F1:  0.26746506986027946\n"
     ]
    }
   ],
   "source": [
    "# Also determine accuracy and macro F1 for a most-frequent class baseline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "baseline_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "baseline_clf.fit(train_vecs, train_klasses)\n",
    "baseline_results = baseline_clf.predict(test_vecs)\n",
    "\n",
    "acc = sklearn.metrics.accuracy_score(test_klasses, baseline_results)\n",
    "f1 = sklearn.metrics.f1_score(test_klasses, baseline_results, average='macro')\n",
    "\n",
    "print(\"Baseline accuracy: \", acc) \n",
    "print(\"Baseline macro F1: \", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f4984",
   "metadata": {},
   "source": [
    "# Submitting your work\n",
    "\n",
    "When you're done, submit a3.ipynb to the assignment 3 folder on D2L."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
